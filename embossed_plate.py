# -*- coding: utf-8 -*-
"""Embossed_plate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k5AaU34ByXEsGqt-hhOEbLP1XvvCWNj1

<center><strong><h1>licence plate detection
"""

import matplotlib.pyplot as plt
import cv2
import numpy as np
import os
import glob
import matplotlib.pyplot as plt
from PIL import Image
plt.style.use('default')  #using black for backgrounf

"""##Setting up kaggle API to download dataset directly from their Website"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

#Change the permission
!chmod 600 ~/.kaggle/kaggle.json

"""## Downloading the datasets"""

#The big dataset
!kaggle datasets download -d xairete/car-plates-ocr

"""### Unzipping"""

from zipfile import ZipFile
file_name="car-plates-ocr.zip"

with ZipFile(file_name,"r") as zip:
  zip.extractall()
  print("Done")

"""## Data Preperation"""

!mkdir "/content/data/train_jpg/"
!mv "/content/data/train/"*.jpg "/content/data/train_jpg/"

#move a sample 1000 out of 17000 to do check
!mkdir "/content/data/train_3000/"
!ls /content/data/train_jpg/* | head -3000 | xargs -I{} cp {} /content/data/train_3000/

"""## Appending Images to X[] (big datatset)

### we need anotations before
"""

import json
with open("/content/data/train.json", "r") as read_file:
    data = json.load(read_file)

IMAGE_SIZE = 224

img_dir = "/content/data/train_3000" # Enter Directory of all images
data_path = os.path.join(img_dir,'*g')
files = glob.glob(data_path)

#We sort the images in alphabetical order to match them to the xml files containing the annotations of the bounding boxes
files.sort()

X=[]
y=[]
for f1 in files:
    img = cv2.imread(f1)
    height = img.shape[0]
    width = img.shape[1]
    img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))
    X.append(img)
    path_list = f1.split(os.sep)
    file_name=path_list[-1]
    index=int(file_name[:-4])
    y.append([ int(data[index]['nums'][0]["box"][0][0]/(width/IMAGE_SIZE)), int(data[index]['nums'][0]["box"][0][1]/(height/IMAGE_SIZE)), int(data[index]['nums'][0]["box"][2][0]/(width/IMAGE_SIZE)), int(data[index]['nums'][0]["box"][2][1]/(height/IMAGE_SIZE))])

"""## Transforming To Ndarray"""

X=np.array(X)
y=np.array(y)

#Renormalisation
X = X / 255
y = y / 255

"""## Split the data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

"""## Creating the model"""

from keras.models import Sequential, Model
from keras.layers import Dense, Flatten, Input
from keras.applications.vgg16 import VGG16
from sklearn.model_selection import train_test_split
import numpy as np
import cv2
import json
import os
import glob

# Constants
IMAGE_SIZE = 224

# Load Dataset Annotations
with open("/content/data/train.json", "r") as read_file:
    data = json.load(read_file)

# Load Images and Annotations
img_dir = "/content/data/train_3000"
data_path = os.path.join(img_dir, '*g')
files = sorted(glob.glob(data_path))

X, y = [], []
for f1 in files:
    img = cv2.imread(f1)
    height, width = img.shape[:2]
    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))
    X.append(img)

    # Obtain file index for bounding box labels
    file_name = os.path.basename(f1)
    index = int(file_name[:-4])  # Adjust if file names are not purely numbers

    # Normalize bounding box coordinates to new image size
    box = data[index]['nums'][0]["box"]
    y.append([
        int(box[0][0] / (width / IMAGE_SIZE)),
        int(box[0][1] / (height / IMAGE_SIZE)),
        int(box[2][0] / (width / IMAGE_SIZE)),
        int(box[2][1] / (height / IMAGE_SIZE))
    ])

X = np.array(X) / 255.0
y = np.array(y) / 255.0

# Train-Validation-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

# Define Model
# Set up the VGG16 model as a base model
input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) # Define an Input layer for the Functional API
base_model = VGG16(weights="imagenet", include_top=False, input_tensor= input_tensor)
base_model.trainable = False  # Freeze the base model

x = base_model.output
x = Flatten()(x) # Use Functional API syntax for layer connections
x = Dense(128, activation="relu")(x)
x = Dense(128, activation="relu")(x)
x = Dense(64, activation="relu")(x)
predictions = Dense(4, activation="sigmoid")(x)

model = Model(inputs=base_model.input, outputs=predictions) # Define the model using Input and output layers

# Show Model Summary
model.summary()
# # Create a new model with the VGG16 base
# model = Sequential([
#     base_model,
#     Flatten(),
#     Dense(128, activation="relu"),
#     Dense(128, activation="relu"),
#     Dense(64, activation="relu"),
#     Dense(4, activation="sigmoid")
# ])

# # Show Model Summary
# model.summary()

# Create the model
model = Sequential()
model.add(VGG16(weights="imagenet", include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))
model.add(Flatten())
model.add(Dense(128, activation="relu"))
model.add(Dense(128, activation="relu"))
model.add(Dense(64, activation="relu"))
model.add(Dense(4, activation="sigmoid"))

model.layers[-6].trainable = False

model.summary()

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])

#training the model
train = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=1)

"""<h1> Testing"""

# Test
scores = model.evaluate(X_test, y_test, verbose=0)
print("Score : %.2f%%" % (scores[1]*100))

"""## Plot Accuracy Curve"""

def plot_scores(train) :
    accuracy = train.history['accuracy']
    val_accuracy = train.history['val_accuracy']
    epochs = range(len(accuracy))
    plt.plot(epochs, accuracy, 'b', label='Training Accuracy')
    plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')
    plt.title('Accuracy')
    plt.legend()
    plt.show()

plot_scores(train)

"""<h2> Export the model"""

model.save('my_model.h5')

##### To load
# new_model = tf.keras.models.load_model('saved_model/my_model')

!sudo apt-get install tesseract-ocr
!pip install pytesseract

from keras.models import load_model
import cv2
import numpy as np
import matplotlib.pyplot as plt
import pytesseract

# Load the saved model
model = load_model('my_model.h5')

# Function to preprocess image for bounding box prediction
def preprocess_image(image_path, target_size=(224, 224)):
    image = cv2.imread(image_path)
    image = cv2.resize(image, target_size)
    image = image / 255.0  # Normalize as done during training
    image = np.expand_dims(image, axis=0)  # Add batch dimension
    return image

# Function to detect bounding box of license plate, using the full image size
def predict_bounding_box(image_path):
    original_image = cv2.imread(image_path)
    height, width = original_image.shape[:2]

    # Use the entire image size as the bounding box
    x_min, y_min = 0, 0
    x_max, y_max = width, height

    return original_image, (x_min, y_min, x_max, y_max)

# Function to extract license plate text using Tesseract OCR
def extract_plate_text(image_path):
    # Predict bounding box for license plate using the full image size
    original_image, bbox = predict_bounding_box(image_path)
    x_min, y_min, x_max, y_max = bbox

    # Draw the bounding box on the original image for visualization
    image_with_box = original_image.copy()
    cv2.rectangle(image_with_box, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)  # Green box
    plt.imshow(cv2.cvtColor(image_with_box, cv2.COLOR_BGR2RGB))
    plt.title("Detected License Plate Area")
    plt.axis('off')
    plt.show()

    # Crop the entire image as the license plate area
    license_plate = original_image[y_min:y_max, x_min:x_max]

    # Convert the cropped image to grayscale for better OCR accuracy
    gray_plate = cv2.cvtColor(license_plate, cv2.COLOR_BGR2GRAY)

    # Apply OCR on the cropped license plate image
    plate_text = pytesseract.image_to_string(gray_plate, config='--psm 7 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ')

    # Display the cropped license plate and OCR result
    plt.imshow(cv2.cvtColor(license_plate, cv2.COLOR_BGR2RGB))
    plt.title(f"Detected Plate Text: {plate_text.strip()}")
    plt.axis('off')
    plt.show()

    return plate_text.strip()

# Use the function on the uploaded image
image_path = '/content/nepal4.jpg'
text_result = extract_plate_text(image_path)  # No padding needed since we use the entire image
print("License Plate Text:", text_result)

